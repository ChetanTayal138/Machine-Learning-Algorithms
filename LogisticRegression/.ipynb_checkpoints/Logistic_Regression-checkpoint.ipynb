{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from math import e\n",
    "from scipy import optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "\n",
    "    def __init__(self,csv_file , feature = False):\n",
    "        data = np.loadtxt(csv_file , delimiter = ',')\n",
    "        X = data[:,:-1]\n",
    "        if feature == False:\n",
    "            self.X = np.insert(X,0,values = np.ones(X.shape[0]) , axis = 1)\n",
    "        else:\n",
    "            self.X = X\n",
    "        self.y = data[:,-1]\n",
    "        \n",
    "        \n",
    "    def mapFeature(self,X1, X2, degree=6):\n",
    "        X1 = np.array(X1)\n",
    "        X2 = np.array(X2)\n",
    "        if X1.ndim > 0:\n",
    "            out = [np.ones(X1.shape[0])]\n",
    "        else:\n",
    "            out = [np.ones(1)]\n",
    "\n",
    "        for i in range(1, degree + 1):\n",
    "            for j in range(i + 1):\n",
    "                out.append((X1 ** (i - j)) * (X2 ** j))\n",
    "\n",
    "        if X1.ndim > 0:\n",
    "            return np.stack(out, axis=1)\n",
    "        else:\n",
    "            return np.array(out)\n",
    "        \n",
    "\n",
    "    def _sigmoid(self,z):\n",
    "        return (1)/(1+e**(-z))\n",
    "\n",
    "    def computeCost(self,theta):\n",
    "        m = self.y.size\n",
    "        grad = np.zeros(self.X.shape[1])\n",
    "        HTheta_X = self._sigmoid(np.dot(self.X,theta))\n",
    "        J = -(1/m)* np.sum(np.dot(self.y,np.log(HTheta_X)) + np.dot((1-self.y) , np.log(1-HTheta_X)))\n",
    "        for i in range(self.X.shape[1]):\n",
    "            grad[i] = grad[i] - (1/m)*np.sum(np.dot(HTheta_X-self.y, self.X[:,i]))\n",
    "            \n",
    "        return J,-grad\n",
    "\n",
    "    def computeCostReg(self,theta):\n",
    "        m = self.y.size \n",
    "        grad = np.zeros(self.X.shape[1])\n",
    "        HTheta_X =self._sigmoid(np.dot(self.X,theta))\n",
    "        J = -(1/m)*(np.sum(  np.dot(self.y,np.log(HTheta_X)) + np.dot(1-self.y,np.log(1-HTheta_X)) ) ) + (lambda_/2*m)*(np.sum(theta**2))\n",
    "    \n",
    "        for i in range(X.shape[1]):\n",
    "            grad[i] = grad[i] - (1/m)*(np.sum(np.dot(HTheta_X-y , X[:,i])) + (lambda_*theta[i]))\n",
    "            \n",
    "        return J , -grad \n",
    "        \n",
    "    \n",
    "    \"\"\"\n",
    "    Explicitly passing the feature and target set for the following functions.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def CostFunction(self,theta,X,y):\n",
    "        m = y.size\n",
    "        grad = np.zeros(X.shape[1])\n",
    "        HTheta_X = self._sigmoid(np.dot(X,theta))\n",
    "        J = -(1/m)* np.sum(np.dot(y,np.log(HTheta_X)) + np.dot((1-y) , np.log(1-HTheta_X)))\n",
    "        for i in range(X.shape[1]):\n",
    "            grad[i] = grad[i] - (1/m)*np.sum(np.dot(HTheta_X-y, X[:,i]))\n",
    "            \n",
    "        return J,-grad\n",
    "    \n",
    "    \n",
    "    def CostFunctionReg(self,theta,X,y,lambda_):\n",
    "        m = y.size \n",
    "        grad = np.zeros(X.shape[1])\n",
    "        HTheta_X = self._sigmoid(np.dot(X,theta))\n",
    "        J = -(1/m)*(np.sum(  np.dot(y,np.log(HTheta_X)) + np.dot(1-y,np.log(1-HTheta_X)) ) ) + (lambda_/2*m)*(np.sum(theta**2))\n",
    "    \n",
    "        for i in range(X.shape[1]):\n",
    "            grad[i] = grad[i] - (1/m)*(np.sum(np.dot(HTheta_X-y , X[:,i])) + (lambda_*theta[i]))\n",
    "            \n",
    "        return J , -grad \n",
    "    \n",
    "        \n",
    "    def optimise(self , theta,reg=False, lambda_ = None):\n",
    "        \n",
    "        \n",
    "        if reg == False:\n",
    "            res = optimize.minimize(self.CostFunction,theta,(self.X,self.y),jac=True,method='TNC',options = {'maxiter':400})\n",
    "        else:\n",
    "            res = optimize.minimize(self.CostFunctionReg ,theta,(self.X,self.y,lambda_),jac=True,method='TNC',options = {'maxiter':100})\n",
    "        cost = res.fun\n",
    "        theta = res.x\n",
    "        return cost,theta\n",
    "    \n",
    "\n",
    "    \"\"\"Gives the training accuracy of our model\"\"\"\n",
    "\n",
    "    def train(self,theta , threshold):\n",
    "        HTheta_X = self._sigmoid(np.dot(self.X,theta))\n",
    "        p = (HTheta_X >= threshold).astype(int)\n",
    "        return p \n",
    "\n",
    "    \"\"\"Simply pass features to be predicted and will return probability of a truth value\"\"\"\n",
    "    \n",
    "    def predict(self,values,theta):\n",
    "        values = np.array([1,*values])\n",
    "        return self._sigmoid(np.dot(values,theta))\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7762906241525984\n",
      "0.6931\n",
      "0.0085 0.0188 0.0001 0.0503 0.0115\n",
      "0.4308 0.1614 0.1948 0.2269 0.0922\n"
     ]
    }
   ],
   "source": [
    "LG = LogisticRegression('ex2data1.txt')\n",
    "reduced_cost , reduced_theta = LG.optimise(theta = np.zeros(LG.X.shape[1]))\n",
    "prediction = LG.predict([45,85] , reduced_theta)\n",
    "print(prediction) #Expected 0.775 +/- 0.002\n",
    "\n",
    "\n",
    "LG2 = LogisticRegression('ex2data2.txt' , feature = True)\n",
    "LG2.X = LG2.mapFeature(LG2.X[:,0] , LG2.X[:,1])\n",
    "\n",
    "initial_theta = np.zeros(LG2.X.shape[1])\n",
    "cost , grad = LG2.CostFunctionReg(initial_theta , LG2.X,LG2.y,1)\n",
    "print(cost.round(4)) #Expected 0.693\n",
    "print(*grad[:5].round(4)) #Expected gradients (approx) - first five values only: [0.0085, 0.0188, 0.0001, 0.0503, 0.0115]\n",
    "\n",
    "\n",
    "test_theta = np.ones(LG2.X.shape[1])\n",
    "cost, grad = LG2.CostFunctionReg(test_theta, LG2.X, LG2.y, 10)\n",
    "print(*grad[:5].round(4)) #Expected gradients (approx) - first five values only: [0.3460, 0.1614, 0.1948, 0.2269, 0.0922]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
